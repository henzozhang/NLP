{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24556038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sentences = [\n",
    "    'i love my dog',\n",
    "    'I, love my cat',\n",
    "    'You love my dog!'\n",
    "]\n",
    "\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a7377",
   "metadata": {},
   "source": [
    "# La tokenisation\n",
    "La tokenisation permet de diviser un texte par mot ou par phrase. Cela vous permet de travailler avec des morceaux de texte plus petits qui sont encore relativement cohérents et significatifs, même en dehors du contexte du reste du texte. C'est la première étape pour transformer des données non structurées en données structurées, plus faciles à analyser.\n",
    "\n",
    "Lorsque vous analysez un texte, vous procédez à une tokenisation par mot et à une tokenisation par phrase. Voici ce que les deux types de tokenisation apportent :\n",
    "\n",
    "## Tokénisation par mot :\n",
    "les mots sont comme les atomes du langage naturel. Ils constituent la plus petite unité de signification qui ait encore un sens en soi. La tokenisation de votre texte par mot vous permet d'identifier les mots qui reviennent particulièrement souvent. Par exemple, si vous analysez un groupe d'offres d'emploi, vous constaterez peut-être que le mot \"Python\" revient souvent. Cela pourrait suggérer une forte demande de connaissances en Python, mais il faudrait aller plus loin pour en savoir plus.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e7a2aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bonjour' 'à' 'tous' 'ceci' 'est' 'un' 'exemple' 'de' 'tokenisation']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'word_tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize(text)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens)\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mword_tokenize\u001b[49m(text))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize(text):\n",
    "    # Convertir le texte en minuscules\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Supprimer les caractères spéciaux et les chiffres\n",
    "    text = ''.join(char for char in text if char.isalpha() or char.isspace())\n",
    "    \n",
    "    # Diviser le texte en mots\n",
    "    words = text.split()\n",
    "    \n",
    "    # Créer un tableau NumPy pour stocker les tokens\n",
    "    tokens = np.array(words)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Exemple d'utilisation\n",
    "text = \"Bonjour à tous ! Ceci est un exemple de tokenisation.\"\n",
    "tokens = tokenize(text)\n",
    "print(tokens)\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82df3202",
   "metadata": {},
   "source": [
    "## Tokénisation par phrase :\n",
    "La tokenisation par phrase permet d'analyser les relations entre les mots et de mieux cerner le contexte. Y a-t-il beaucoup de mots négatifs autour du mot \"Python\" parce que le responsable du recrutement n'aime pas Python ? Y a-t-il plus de termes du domaine de l'herpétologie que du domaine du développement logiciel, ce qui suggère que vous avez peut-être affaire à un type de Python totalement différent de ce à quoi vous vous attendiez ?\n",
    "\n",
    "Voici comment importer les parties pertinentes de NLTK afin de pouvoir procéder à une tokenisation par mot et par phrase :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c39b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bonjour à tous ! Ceci est un exemple de tokenisation'\n",
      " 'Une autre phrase ici']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def tokenize_sentences(text):\n",
    "    # Diviser le texte en phrases en utilisant les points comme séparateurs\n",
    "    sentences = text.split('.')\n",
    "    \n",
    "    # Nettoyer les espaces vides et les phrases vides\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    # Créer un tableau NumPy pour stocker les phrases\n",
    "    tokens = np.array(sentences)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Exemple d'utilisation\n",
    "text = \"Bonjour à tous ! Ceci est un exemple de tokenisation. Une autre phrase ici.\"\n",
    "tokens = tokenize_sentences(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8ba9741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38211df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_string = \"\"\"\n",
    "Le NLP révolutionne notre façon de comprendre le langage humain.\n",
    "En utilisant des techniques avancées de traitement de texte,\n",
    "nous pouvons extraire des informations précieuses à partir de vastes ensembles de données textuelles.\n",
    "La tokenisation, première étape cruciale du NLP, consiste à diviser le texte en unités lexicales,\n",
    "telles que des mots ou des phrases. Cette démarche permet une analyse plus approfondie du langage,\n",
    "facilitant ainsi la traduction automatique, l'analyse des sentiments et bien d'autres tâches.\n",
    "Grâce à la tokenisation, nous ouvrons la voie à une meilleure compréhension et interaction avec le langage naturel.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ded2cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenize(example_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25daeeae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_tokenize(example_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd55812d",
   "metadata": {},
   "source": [
    "# application\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e80cacb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ouverture du fichier en mode lecture ligne par ligne\n",
    "liste_words=[]\n",
    "liste_phrase=[]\n",
    "mon_fichier=\"French/9-11-in-perspective.txt\"\n",
    "with open(mon_fichier, 'r', encoding='UTF-8') as file:\n",
    "    # Lire le contenu ligne par ligne\n",
    "    lignes = file.readlines()\n",
    "    \n",
    "    lignes = ''.join(char for char in lignes if char.isalpha() or char.isspace())\n",
    "    # Afficher chaque ligne\n",
    "lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6cd5c2",
   "metadata": {},
   "source": [
    "# Filtrage des stop word\n",
    "Les mots vides sont des mots que vous souhaitez ignorer et que vous filtrez donc dans votre texte lorsque vous le traitez. Des mots très courants comme \" in \", \" is \" et \" an \" sont souvent utilisés comme mots vides car ils n'ajoutent pas beaucoup de sens à un texte en eux-mêmes.\n",
    "\n",
    "Voici comment importer les parties pertinentes de NLTK afin de filtrer les mots vides :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46a4531b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\gcher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk as nltk\n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f1312ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "worf_quote = \"Sir, I protest. I am not a merry man!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89d723a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'I', 'protest', '.', 'I', 'am', 'not', 'a', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_quote = word_tokenize(worf_quote)\n",
    "words_in_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5af8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a16b3436",
   "metadata": {},
   "source": [
    "Vous disposez d'une liste de mots dans worf_quote, l'étape suivante consiste donc à créer un ensemble de stop words pour filtrer words_in_quote. Pour cet exemple, vous devrez vous concentrer sur les stop words de \"english\" :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b206ef6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words= set(stopwords.words(\"english\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9aefa96",
   "metadata": {},
   "source": [
    "Ensuite, créez une liste vide pour contenir les mots qui ont passé le filtre :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49e0f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16374c4",
   "metadata": {},
   "source": [
    "Vous avez créé une liste vide, filtered_list, pour contenir tous les mots de words_in_quote qui ne sont pas des stopwords. Vous pouvez maintenant utiliser stop_words pour filtrer words_in_quote :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68820ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words_in_quote:\n",
    "    if word.casefold() not in stop_words:\n",
    "        filtered_list.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a96d79",
   "metadata": {},
   "source": [
    "Vous avez itéré sur words_in_quote avec une boucle for et ajouté tous les mots qui n'étaient pas des stop words à filtered_list. Vous avez utilisé .casefold() sur word pour pouvoir ignorer si les lettres du mot étaient en majuscules ou en minuscules. Cela vaut la peine car stopwords.words('english') n'inclut que les versions minuscules des mots vides.\n",
    "\n",
    "Vous pouvez également utiliser une compréhension de liste pour établir une liste de tous les mots de votre texte qui ne sont pas des mots vides :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0243df67",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_list = [\n",
    "    word for word in words_in_quote if word.casefold() not in stop_words\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc983a4",
   "metadata": {},
   "source": [
    "Lorsque vous utilisez une compréhension de liste, vous ne créez pas une liste vide et n'ajoutez pas ensuite des éléments à la fin de celle-ci. Au contraire, vous définissez la liste et son contenu en même temps. L'utilisation d'une compréhension de liste est souvent considérée comme plus pythonique.\n",
    "\n",
    "Jetez un coup d'œil aux mots qui se sont retrouvés dans filtered_list :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a357ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sir', ',', 'protest', '.', 'merry', 'man', '!']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ca5edb",
   "metadata": {},
   "source": [
    "Vous avez filtré quelques mots comme \"am\" et \"a\", mais vous avez également filtré \"not\", ce qui affecte le sens général de la phrase. (Worf ne sera pas content).\n",
    "\n",
    "Les mots comme \"I\" et \"not\" peuvent sembler trop importants pour être éliminés, et selon le type d'analyse que vous voulez faire, ils peuvent l'être. Voici pourquoi :\n",
    "\n",
    "Le \"je\" est un pronom, qui est un mot de contexte plutôt qu'un mot de contenu :\n",
    "\n",
    "Les mots de contenu vous donnent des informations sur les sujets traités dans le texte ou sur le sentiment de l'auteur par rapport à ces sujets.\n",
    "\n",
    "Les mots de contexte vous renseignent sur le style d'écriture. Vous pouvez observer des modèles dans la façon dont les auteurs utilisent les mots de contexte afin de quantifier leur style d'écriture. Une fois que vous avez quantifié leur style d'écriture, vous pouvez analyser un texte écrit par un auteur inconnu pour voir dans quelle mesure il suit un style d'écriture particulier afin d'essayer d'identifier l'auteur.\n",
    "\n",
    "'not' est techniquement un adverbe mais a tout de même été inclus dans la liste des mots vides de NLTK pour l'anglais. Si vous souhaitez modifier la liste des mots interdits pour exclure 'not' ou faire d'autres changements, vous pouvez la télécharger.\n",
    "\n",
    "Ainsi, \"I\" et \"not\" peuvent être des parties importantes d'une phrase, mais cela dépend de ce que vous essayez d'apprendre de cette phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e11b0b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68326a1e",
   "metadata": {},
   "source": [
    "# Stemming\n",
    "\n",
    "Le stemming est une tâche de traitement de texte qui consiste à réduire les mots à leur racine, qui est la partie centrale du mot. Par exemple, les mots \"helping\" et \"helper\" partagent la racine \"help\". Le stemming vous permet de vous concentrer sur le sens fondamental d'un mot plutôt que sur tous les détails de son utilisation. NLTK dispose de plusieurs logiciels de troncature, mais vous utiliserez le logiciel de troncature Porter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c47fe69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf571fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer() #create a stemmer with PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ba76dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_stemming = \"\"\"\n",
    "The crew of the USS Discovery discovered many discoveries.\n",
    "Discovering is what explorers do.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc584da",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7b5b3b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'USS',\n",
       " 'Discovery',\n",
       " 'discovered',\n",
       " 'many',\n",
       " 'discoveries',\n",
       " '.',\n",
       " 'Discovering',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explorers',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2fdf33f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_words = [stemmer.stem(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "91ff5a23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'crew',\n",
       " 'of',\n",
       " 'the',\n",
       " 'uss',\n",
       " 'discoveri',\n",
       " 'discov',\n",
       " 'mani',\n",
       " 'discoveri',\n",
       " '.',\n",
       " 'discov',\n",
       " 'is',\n",
       " 'what',\n",
       " 'explor',\n",
       " 'do',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b612506",
   "metadata": {},
   "source": [
    "<div class=\"table-responsive\">\n",
    "<table class=\"table table-hover\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Original word</th>\n",
    "<th>Stemmed version</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td><code>'Discovery'</code></td>\n",
    "<td><code>'discoveri'</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>'discovered'</code></td>\n",
    "<td><code>'discov'</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>'discoveries'</code></td>\n",
    "<td><code>'discoveri'</code></td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td><code>'Discovering'</code></td>\n",
    "<td><code>'discov'</code></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7574f7f",
   "metadata": {},
   "source": [
    "Ces résultats semblent quelque peu incohérents. Pourquoi \"Discovery\" donnerait-il \"discoveri\" alors que \"Discovering\" donnerait \"discov\" ?\n",
    "\n",
    "Understemming  et la overstemming  sont deux façons dont le stemming peut se tromper :\n",
    "\n",
    "La sous-évaluation se produit lorsque deux mots apparentés devraient être réduits à la même racine, mais ne le sont pas. Il s'agit d'un faux négatif.\n",
    "Il y a sur-témoignage lorsque deux mots non apparentés sont réduits au même radical alors qu'ils ne devraient pas l'être. Il s'agit d'un faux positif.\n",
    "L'algorithme d'extraction de Porter date de 1979, il est donc un peu plus ancien. Le troncatureur Snowball, également appelé Porter2, est une amélioration de l'original et est également disponible via NLTK, vous pouvez donc l'utiliser dans vos propres projets. Il convient également de noter que l'objectif du troncteur Porter n'est pas de produire des mots complets, mais de trouver des formes variantes d'un mot.\n",
    "\n",
    "Heureusement, il existe d'autres moyens de réduire les mots à leur sens principal, comme la lemmatisation, que vous verrez plus loin dans ce tutoriel. Mais tout d'abord, nous devons aborder la question des parties du discours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898dff8",
   "metadata": {},
   "source": [
    "# Étiquetage des parties du discours"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b27df50",
   "metadata": {},
   "source": [
    "La partie du discours est un terme grammatical qui traite des rôles joués par les mots lorsqu'ils sont utilisés ensemble dans des phrases. L'étiquetage des parties du discours, ou étiquetage POS, consiste à étiqueter les mots de votre texte en fonction de leur partie du discours.\n",
    "\n",
    "En anglais, il existe huit parties du discours :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88758b",
   "metadata": {},
   "source": [
    "<div class=\"table-responsive\">\n",
    "<table class=\"table table-hover\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>Part of speech</th>\n",
    "<th>Role</th>\n",
    "<th>Examples</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>Noun</td>\n",
    "<td>Is a person, place, or thing</td>\n",
    "<td>mountain, bagel, Poland</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Pronoun</td>\n",
    "<td>Replaces a noun</td>\n",
    "<td>you, she, we</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Adjective</td>\n",
    "<td>Gives information about what a noun is like</td>\n",
    "<td>efficient, windy, colorful</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Verb</td>\n",
    "<td>Is an action or a state of being</td>\n",
    "<td>learn, is, go</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Adverb</td>\n",
    "<td>Gives information about a verb, an adjective, or another adverb</td>\n",
    "<td>efficiently, always, very</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Preposition</td>\n",
    "<td>Gives information about how a noun or pronoun is connected to another word</td>\n",
    "<td>from, about, at</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Conjunction</td>\n",
    "<td>Connects two other words or phrases</td>\n",
    "<td>so, because, and</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>Interjection</td>\n",
    "<td>Is an exclamation</td>\n",
    "<td>yay, ow, wow</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "116b2246",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagan_quote = \"\"\"\n",
    "... If you wish to make an apple pie from scratch,\n",
    "... you must first invent the universe.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "978fb0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_sagan_quote = word_tokenize(sagan_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "afb18e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gcher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('If', 'IN'),\n",
       " ('you', 'PRP'),\n",
       " ('wish', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('make', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('apple', 'NN'),\n",
       " ('pie', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('scratch', 'NN'),\n",
       " (',', ','),\n",
       " ('you', 'PRP'),\n",
       " ('must', 'MD'),\n",
       " ('first', 'VB'),\n",
       " ('invent', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.pos_tag(words_in_sagan_quote)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203ac586",
   "metadata": {},
   "source": [
    "Tous les mots de la citation se trouvent désormais dans un tuple distinct, avec une balise qui représente leur partie du discours. Mais que signifient les balises ? Voici comment obtenir une liste des balises et de leur signification :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c41ff07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$: dollar\n",
      "    $ -$ --$ A$ C$ HK$ M$ NZ$ S$ U.S.$ US$\n",
      "'': closing quotation mark\n",
      "    ' ''\n",
      "(: opening parenthesis\n",
      "    ( [ {\n",
      "): closing parenthesis\n",
      "    ) ] }\n",
      ",: comma\n",
      "    ,\n",
      "--: dash\n",
      "    --\n",
      ".: sentence terminator\n",
      "    . ! ?\n",
      ":: colon or ellipsis\n",
      "    : ; ...\n",
      "CC: conjunction, coordinating\n",
      "    & 'n and both but either et for less minus neither nor or plus so\n",
      "    therefore times v. versus vs. whether yet\n",
      "CD: numeral, cardinal\n",
      "    mid-1890 nine-thirty forty-two one-tenth ten million 0.5 one forty-\n",
      "    seven 1987 twenty '79 zero two 78-degrees eighty-four IX '60s .025\n",
      "    fifteen 271,124 dozen quintillion DM2,000 ...\n",
      "DT: determiner\n",
      "    all an another any both del each either every half la many much nary\n",
      "    neither no some such that the them these this those\n",
      "EX: existential there\n",
      "    there\n",
      "FW: foreign word\n",
      "    gemeinschaft hund ich jeux habeas Haementeria Herr K'ang-si vous\n",
      "    lutihaw alai je jour objets salutaris fille quibusdam pas trop Monte\n",
      "    terram fiche oui corporis ...\n",
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n",
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n",
      "JJR: adjective, comparative\n",
      "    bleaker braver breezier briefer brighter brisker broader bumper busier\n",
      "    calmer cheaper choosier cleaner clearer closer colder commoner costlier\n",
      "    cozier creamier crunchier cuter ...\n",
      "JJS: adjective, superlative\n",
      "    calmest cheapest choicest classiest cleanest clearest closest commonest\n",
      "    corniest costliest crassest creepiest crudest cutest darkest deadliest\n",
      "    dearest deepest densest dinkiest ...\n",
      "LS: list item marker\n",
      "    A A. B B. C C. D E F First G H I J K One SP-44001 SP-44002 SP-44005\n",
      "    SP-44007 Second Third Three Two * a b c d first five four one six three\n",
      "    two\n",
      "MD: modal auxiliary\n",
      "    can cannot could couldn't dare may might must need ought shall should\n",
      "    shouldn't will would\n",
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n",
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "NNPS: noun, proper, plural\n",
      "    Americans Americas Amharas Amityvilles Amusements Anarcho-Syndicalists\n",
      "    Andalusians Andes Andruses Angels Animals Anthony Antilles Antiques\n",
      "    Apache Apaches Apocrypha ...\n",
      "NNS: noun, common, plural\n",
      "    undergraduates scotches bric-a-brac products bodyguards facets coasts\n",
      "    divestitures storehouses designs clubs fragrances averages\n",
      "    subjectivists apprehensions muses factory-jobs ...\n",
      "PDT: pre-determiner\n",
      "    all both half many quite such sure this\n",
      "POS: genitive marker\n",
      "    ' 's\n",
      "PRP: pronoun, personal\n",
      "    hers herself him himself hisself it itself me myself one oneself ours\n",
      "    ourselves ownself self she thee theirs them themselves they thou thy us\n",
      "PRP$: pronoun, possessive\n",
      "    her his mine my our ours their thy your\n",
      "RB: adverb\n",
      "    occasionally unabatingly maddeningly adventurously professedly\n",
      "    stirringly prominently technologically magisterially predominately\n",
      "    swiftly fiscally pitilessly ...\n",
      "RBR: adverb, comparative\n",
      "    further gloomier grander graver greater grimmer harder harsher\n",
      "    healthier heavier higher however larger later leaner lengthier less-\n",
      "    perfectly lesser lonelier longer louder lower more ...\n",
      "RBS: adverb, superlative\n",
      "    best biggest bluntest earliest farthest first furthest hardest\n",
      "    heartiest highest largest least less most nearest second tightest worst\n",
      "RP: particle\n",
      "    aboard about across along apart around aside at away back before behind\n",
      "    by crop down ever fast for forth from go high i.e. in into just later\n",
      "    low more off on open out over per pie raising start teeth that through\n",
      "    under unto up up-pp upon whole with you\n",
      "SYM: symbol\n",
      "    % & ' '' ''. ) ). * + ,. < = > @ A[fj] U.S U.S.S.R * ** ***\n",
      "TO: \"to\" as preposition or infinitive marker\n",
      "    to\n",
      "UH: interjection\n",
      "    Goodbye Goody Gosh Wow Jeepers Jee-sus Hubba Hey Kee-reist Oops amen\n",
      "    huh howdy uh dammit whammo shucks heck anyways whodunnit honey golly\n",
      "    man baby diddle hush sonuvabitch ...\n",
      "VB: verb, base form\n",
      "    ask assemble assess assign assume atone attention avoid bake balkanize\n",
      "    bank begin behold believe bend benefit bevel beware bless boil bomb\n",
      "    boost brace break bring broil brush build ...\n",
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n",
      "VBG: verb, present participle or gerund\n",
      "    telegraphing stirring focusing angering judging stalling lactating\n",
      "    hankerin' alleging veering capping approaching traveling besieging\n",
      "    encrypting interrupting erasing wincing ...\n",
      "VBN: verb, past participle\n",
      "    multihulled dilapidated aerosolized chaired languished panelized used\n",
      "    experimented flourished imitated reunifed factored condensed sheared\n",
      "    unsettled primed dubbed desired ...\n",
      "VBP: verb, present tense, not 3rd person singular\n",
      "    predominate wrap resort sue twist spill cure lengthen brush terminate\n",
      "    appear tend stray glisten obtain comprise detest tease attract\n",
      "    emphasize mold postpone sever return wag ...\n",
      "VBZ: verb, present tense, 3rd person singular\n",
      "    bases reconstructs marks mixes displeases seals carps weaves snatches\n",
      "    slumps stretches authorizes smolders pictures emerges stockpiles\n",
      "    seduces fizzes uses bolsters slaps speaks pleads ...\n",
      "WDT: WH-determiner\n",
      "    that what whatever which whichever\n",
      "WP: WH-pronoun\n",
      "    that what whatever whatsoever which who whom whosoever\n",
      "WP$: WH-pronoun, possessive\n",
      "    whose\n",
      "WRB: Wh-adverb\n",
      "    how however whence whenever where whereby whereever wherein whereof why\n",
      "``: opening quotation mark\n",
      "    ` ``\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to\n",
      "[nltk_data]     C:\\Users\\gcher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package tagsets is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5276350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jabberwocky_excerpt = \"\"\"\n",
    "'Twas brillig, and the slithy toves did gyre and gimble in the wabe:\n",
    "all mimsy were the borogoves, and the mome raths outgrabe.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f18f632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words_in_excerpt = word_tokenize(jabberwocky_excerpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79c7e7ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"'Twas\", 'CD'),\n",
       " ('brillig', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('slithy', 'JJ'),\n",
       " ('toves', 'NNS'),\n",
       " ('did', 'VBD'),\n",
       " ('gyre', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('gimble', 'JJ'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('wabe', 'NN'),\n",
       " (':', ':'),\n",
       " ('all', 'DT'),\n",
       " ('mimsy', 'NNS'),\n",
       " ('were', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('borogoves', 'NNS'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('mome', 'JJ'),\n",
       " ('raths', 'NNS'),\n",
       " ('outgrabe', 'RB'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(words_in_excerpt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64bd116",
   "metadata": {},
   "source": [
    "# La lemmatisation\n",
    "Maintenant que vous connaissez les parties du discours, vous pouvez revenir à la lemmatisation. Comme la troncature, la lemmatisation réduit les mots à leur sens principal, mais elle vous donnera un mot anglais complet qui a un sens en lui-même au lieu d'un fragment de mot comme \"discoveri\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "778ce71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\gcher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f117ad53",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() #Create a lemmatizer to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ac623672",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'scarf'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"scarves\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1a784dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "string_for_lemmatizing = \"The friends of DeSoto love scarves.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "801992ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(string_for_lemmatizing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "49e75b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8642c77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'friend', 'of', 'DeSoto', 'love', 'scarf', '.']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42ee20e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bad'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"worst\", pos=\"a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ce9a15",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "Alors que la segmentation permet d'identifier les mots et les phrases, le découpage permet d'identifier les expressions.\n",
    "\n",
    "Remarque : un syntagme est un mot ou un groupe de mots qui fonctionne comme une unité unique pour remplir une fonction grammaticale. Les locutions nominales sont construites autour d'un nom.\n",
    "\n",
    "En voici quelques exemples :\n",
    "\n",
    "\"Une planète\"\n",
    "\"Une planète qui s'incline\n",
    "\"Une planète qui s'incline rapidement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3ecaf",
   "metadata": {},
   "source": [
    "Le découpage en morceaux utilise les étiquettes POS pour regrouper les mots et appliquer les étiquettes de morceaux à ces groupes. Les morceaux ne se chevauchent pas, de sorte qu'une instance d'un mot ne peut se trouver que dans un seul morceau à la fois.\n",
    "\n",
    "Voici comment importer les parties pertinentes de NLTK afin de procéder au découpage :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2e100ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lotr_quote = \"It's a dangerous business, Frodo, going out your door.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6f96d89d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It',\n",
       " \"'s\",\n",
       " 'a',\n",
       " 'dangerous',\n",
       " 'business',\n",
       " ',',\n",
       " 'Frodo',\n",
       " ',',\n",
       " 'going',\n",
       " 'out',\n",
       " 'your',\n",
       " 'door',\n",
       " '.']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_in_lotr_quote = word_tokenize(lotr_quote)\n",
    "words_in_lotr_quote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eac4c4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\gcher\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('dangerous', 'JJ'),\n",
       " ('business', 'NN'),\n",
       " (',', ','),\n",
       " ('Frodo', 'NNP'),\n",
       " (',', ','),\n",
       " ('going', 'VBG'),\n",
       " ('out', 'RP'),\n",
       " ('your', 'PRP$'),\n",
       " ('door', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "lotr_pos_tags = nltk.pos_tag(words_in_lotr_quote)\n",
    "lotr_pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8da10d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP: {<DT>?<JJ>*<NN>}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "118779e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1a9effa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 1 stages>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ceb1eda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = chunk_parser.parse(lotr_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "16b5eead",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b528bd",
   "metadata": {},
   "source": [
    "# Chinking\n",
    "Le chinking est utilisé conjointement avec le chunking, mais alors que le chunking est utilisé pour inclure un motif, le chinking est utilisé pour exclure un motif.\n",
    "\n",
    "Réutilisons la citation que vous avez utilisée dans la section sur le découpage. Vous disposez déjà d'une liste de tuples contenant chacun des mots de la citation ainsi que sa balise de partie du discours :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4b5d1c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('It', 'PRP'),\n",
       " (\"'s\", 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('dangerous', 'JJ'),\n",
       " ('business', 'NN'),\n",
       " (',', ','),\n",
       " ('Frodo', 'NNP'),\n",
       " (',', ','),\n",
       " ('going', 'VBG'),\n",
       " ('out', 'RP'),\n",
       " ('your', 'PRP$'),\n",
       " ('door', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lotr_pos_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3c046b",
   "metadata": {},
   "source": [
    "L'étape suivante consiste à créer une grammaire pour déterminer ce que vous voulez inclure et exclure dans vos morceaux. Cette fois, vous allez utiliser plus d'une ligne parce que vous allez avoir plus d'une règle. Comme vous utilisez plus d'une ligne pour la grammaire, vous utiliserez des guillemets triples (\"\"\") :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "6ed22704",
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "Chunk: {<.*>+}\n",
    "       }<JJ>{\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eab3619",
   "metadata": {},
   "source": [
    "La première règle de votre grammaire est {<.*>+}. Cette règle a des accolades tournées vers l'intérieur ({}) parce qu'elle est utilisée pour déterminer les motifs que vous voulez inclure dans vos morceaux. Dans ce cas, vous voulez tout inclure : <.*>+.\n",
    "\n",
    "La deuxième règle de votre grammaire est }<JJ>{. Cette règle a des accolades tournées vers l'extérieur (}{) parce qu'elle est utilisée pour déterminer les motifs que vous voulez exclure dans vos morceaux. Dans ce cas, vous voulez exclure les adjectifs : <JJ>.\n",
    "\n",
    "Créez un analyseur de morceaux avec cette grammaire :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bdc22319",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_parser = nltk.RegexpParser(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "85b6bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = chunk_parser.parse(lotr_pos_tags)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb8a2b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805fba54",
   "metadata": {},
   "source": [
    "Ici, vous avez exclu l'adjectif \"dangereux\" de vos morceaux et vous vous retrouvez avec deux morceaux contenant tout le reste. Le premier bloc contient tout le texte apparaissant avant l'adjectif exclu. Le second contient tout ce qui se trouve après l'adjectif exclu.\n",
    "\n",
    "Maintenant que vous savez comment exclure des motifs de vos morceaux, il est temps de vous pencher sur la reconnaissance des entités nommées (NER)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd838ab2",
   "metadata": {},
   "source": [
    "# Utilisation de la reconnaissance des entités nommées Named Entity Recognition (NER)\n",
    "Les entités nommées sont des expressions nominales qui font référence à des lieux, des personnes, des organisations, etc. spécifiques. Grâce à la reconnaissance des entités nommées, vous pouvez trouver les entités nommées dans vos textes et déterminer de quel type d'entité nommée il s'agit.\n",
    "\n",
    "Voici la liste des types d'entités nommées tirée du livre NLTK :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f3a992",
   "metadata": {},
   "source": [
    "<table class=\"table table-hover\">\n",
    "<thead>\n",
    "<tr>\n",
    "<th>NE type</th>\n",
    "<th>Examples</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>ORGANIZATION</td>\n",
    "<td>Georgia-Pacific Corp., WHO</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PERSON</td>\n",
    "<td>Eddy Bonte, President Obama</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>LOCATION</td>\n",
    "<td>Murray River, Mount Everest</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>DATE</td>\n",
    "<td>June, 2008-06-29</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>TIME</td>\n",
    "<td>two fifty a m, 1:30 p.m.</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>MONEY</td>\n",
    "<td>175 million Canadian dollars, GBP 10.40</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>PERCENT</td>\n",
    "<td>twenty pct, 18.75 %</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>FACILITY</td>\n",
    "<td>Washington Monument, Stonehenge</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>GPE</td>\n",
    "<td>South East Asia, Midlothian</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b2cc8900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading maxent_ne_chunker: <urlopen error [Errno\n",
      "[nltk_data]     11001] getaddrinfo failed>\n",
      "[nltk_data] Error loading words: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "tree = nltk.ne_chunk(lotr_pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "f70de243",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83cbeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = nltk.ne_chunk(lotr_pos_tags, binary=True)\n",
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f7af6d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "quote = \"\"\"\n",
    "Men like Schiaparelli watched the red planet—it is odd, by-the-bye, that\n",
    "for countless centuries Mars has been the star of war—but failed to\n",
    "interpret the fluctuating appearances of the markings they mapped so well.\n",
    "All that time the Martians must have been getting ready.\n",
    "\n",
    "During the opposition of 1894 a great light was seen on the illuminated\n",
    "part of the disk, first at the Lick Observatory, then by Perrotin of Nice,\n",
    "and then by other observers. English readers heard of it first in the\n",
    "issue of Nature dated August 2.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2acc894e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ne(quote):\n",
    "    words = word_tokenize(quote, language=\"English\")\n",
    "    tags = nltk.pos_tag(words)\n",
    "    tree = nltk.ne_chunk(tags, binary=True)\n",
    "    return set(\n",
    "        \" \".join(i[0] for i in t)\n",
    "        for t in tree\n",
    "        if hasattr(t, \"label\") and t.label() == \"NE\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "beea425c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Lick Observatory', 'Mars', 'Nature', 'Perrotin', 'Schiaparelli'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ne(quote)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2a9e26c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(quote, language=\"English\")\n",
    "tags = nltk.pos_tag(words)\n",
    "tree = nltk.ne_chunk(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a7b6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a63cb93e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Couldn't find file at https://dumps.wikimedia.org/swwiki/20220120/dumpstatus.json",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[1;32m----> 3\u001b[0m l\u001b[38;5;241m=\u001b[39m\u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20220120\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\load.py:2609\u001b[0m, in \u001b[0;36mload_dataset\u001b[1;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, ignore_verifications, keep_in_memory, save_infos, revision, token, use_auth_token, task, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[0;32m   2606\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[0;32m   2608\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[1;32m-> 2609\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2610\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2611\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2612\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2614\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2615\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2617\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[0;32m   2618\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2619\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[0;32m   2620\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[1;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[0;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1026\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[1;32m-> 1027\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_and_prepare(\n\u001b[0;32m   1028\u001b[0m         dl_manager\u001b[38;5;241m=\u001b[39mdl_manager,\n\u001b[0;32m   1029\u001b[0m         verification_mode\u001b[38;5;241m=\u001b[39mverification_mode,\n\u001b[0;32m   1030\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_split_kwargs,\n\u001b[0;32m   1031\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdownload_and_prepare_kwargs,\n\u001b[0;32m   1032\u001b[0m     )\n\u001b[0;32m   1033\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[0;32m   1034\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\builder.py:1100\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[1;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[0;32m   1098\u001b[0m split_dict \u001b[38;5;241m=\u001b[39m SplitDict(dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name)\n\u001b[0;32m   1099\u001b[0m split_generators_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001b[1;32m-> 1100\u001b[0m split_generators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_generators(dl_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msplit_generators_kwargs)\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;66;03m# Checksums verification\u001b[39;00m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verification_mode \u001b[38;5;241m==\u001b[39m VerificationMode\u001b[38;5;241m.\u001b[39mALL_CHECKS \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mrecord_checksums:\n",
      "File \u001b[1;32m~\\.cache\\huggingface\\modules\\datasets_modules\\datasets\\wikipedia\\d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001\\wikipedia.py:973\u001b[0m, in \u001b[0;36mWikipedia._split_generators\u001b[1;34m(self, dl_manager)\u001b[0m\n\u001b[0;32m    971\u001b[0m info_url \u001b[38;5;241m=\u001b[39m _base_url(lang) \u001b[38;5;241m+\u001b[39m _INFO_FILE\n\u001b[0;32m    972\u001b[0m \u001b[38;5;66;03m# Use dictionary since testing mock always returns the same result.\u001b[39;00m\n\u001b[1;32m--> 973\u001b[0m downloaded_files \u001b[38;5;241m=\u001b[39m \u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minfo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo_url\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    975\u001b[0m xml_urls \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    976\u001b[0m total_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:434\u001b[0m, in \u001b[0;36mDownloadManager.download_and_extract\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_and_extract\u001b[39m(\u001b[38;5;28mself\u001b[39m, url_or_urls):\n\u001b[0;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Download and extract given `url_or_urls`.\u001b[39;00m\n\u001b[0;32m    420\u001b[0m \n\u001b[0;32m    421\u001b[0m \u001b[38;5;124;03m    Is roughly equivalent to:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m        extracted_path(s): `str`, extracted paths of given URL(s).\u001b[39;00m\n\u001b[0;32m    433\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:257\u001b[0m, in \u001b[0;36mDownloadManager.download\u001b[1;34m(self, url_or_urls)\u001b[0m\n\u001b[0;32m    255\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    256\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m stack_multiprocessing_download_progress_bars():\n\u001b[1;32m--> 257\u001b[0m     downloaded_path_or_paths \u001b[38;5;241m=\u001b[39m \u001b[43mmap_nested\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_urls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_tuple\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDownloading data files\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m duration \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[0;32m    267\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading took \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduration\u001b[38;5;241m.\u001b[39mtotal_seconds()\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m min\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:508\u001b[0m, in \u001b[0;36mmap_nested\u001b[1;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy, num_proc, parallel_min_length, batched, batch_size, types, disable_tqdm, desc)\u001b[0m\n\u001b[0;32m    506\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    507\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[1;32m--> 508\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    509\u001b[0m     _single_map_nested((function, obj, batched, batch_size, types, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[0;32m    511\u001b[0m ]\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    513\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:509\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    506\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m num_proc \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(iterable) \u001b[38;5;241m%\u001b[39m num_proc \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    507\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(iter_batched(iterable, batch_size))\n\u001b[0;32m    508\u001b[0m mapped \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m--> 509\u001b[0m     \u001b[43m_single_map_nested\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m hf_tqdm(iterable, disable\u001b[38;5;241m=\u001b[39mdisable_tqdm, desc\u001b[38;5;241m=\u001b[39mdesc)\n\u001b[0;32m    511\u001b[0m ]\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batched:\n\u001b[0;32m    513\u001b[0m     mapped \u001b[38;5;241m=\u001b[39m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m mapped_batch \u001b[38;5;129;01min\u001b[39;00m mapped \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m mapped_batch]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:377\u001b[0m, in \u001b[0;36m_single_map_nested\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    372\u001b[0m     batched\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[0;32m    376\u001b[0m ):\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m function(batch)]\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\py_utils.py:377\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m function(data_struct)\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    372\u001b[0m     batched\n\u001b[0;32m    373\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, \u001b[38;5;28mdict\u001b[39m)\n\u001b[0;32m    374\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_struct, types)\n\u001b[0;32m    375\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(v, types) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m data_struct)\n\u001b[0;32m    376\u001b[0m ):\n\u001b[1;32m--> 377\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [mapped_item \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m iter_batched(data_struct, batch_size) \u001b[38;5;28;01mfor\u001b[39;00m mapped_item \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[0;32m    379\u001b[0m \u001b[38;5;66;03m# Reduce logging to keep things readable in multiprocessing with tqdm\u001b[39;00m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rank \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m logging\u001b[38;5;241m.\u001b[39mget_verbosity() \u001b[38;5;241m<\u001b[39m logging\u001b[38;5;241m.\u001b[39mWARNING:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:313\u001b[0m, in \u001b[0;36mDownloadManager._download_batched\u001b[1;34m(self, url_or_filenames, download_config)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[0;32m    301\u001b[0m         download_func,\n\u001b[0;32m    302\u001b[0m         url_or_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[0;32m    311\u001b[0m     )\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    314\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_download_single(url_or_filename, download_config\u001b[38;5;241m=\u001b[39mdownload_config)\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[0;32m    316\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:314\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m thread_map(\n\u001b[0;32m    301\u001b[0m         download_func,\n\u001b[0;32m    302\u001b[0m         url_or_filenames,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    310\u001b[0m         tqdm_class\u001b[38;5;241m=\u001b[39mtqdm,\n\u001b[0;32m    311\u001b[0m     )\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    313\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 314\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url_or_filename \u001b[38;5;129;01min\u001b[39;00m url_or_filenames\n\u001b[0;32m    316\u001b[0m     ]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\download\\download_manager.py:323\u001b[0m, in \u001b[0;36mDownloadManager._download_single\u001b[1;34m(self, url_or_filename, download_config)\u001b[0m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_relative_path(url_or_filename):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# append the relative path to the base_path\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m url_or_path_join(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path, url_or_filename)\n\u001b[1;32m--> 323\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mcached_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    324\u001b[0m out \u001b[38;5;241m=\u001b[39m tracked_str(out)\n\u001b[0;32m    325\u001b[0m out\u001b[38;5;241m.\u001b[39mset_origin(url_or_filename)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:201\u001b[0m, in \u001b[0;36mcached_path\u001b[1;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[0;32m    197\u001b[0m     url_or_filename \u001b[38;5;241m=\u001b[39m strip_protocol(url_or_filename)\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[1;32m--> 201\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[43mget_from_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_or_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_etag\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_etag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_url_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mignore_url_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisable_tqdm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisable_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(url_or_filename):\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# File, and it exists.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m url_or_filename\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\datasets\\utils\\file_utils.py:630\u001b[0m, in \u001b[0;36mget_from_cache\u001b[1;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only, use_etag, max_retries, token, use_auth_token, ignore_url_params, storage_options, download_desc, disable_tqdm)\u001b[0m\n\u001b[0;32m    625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\n\u001b[0;32m    626\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find the requested files in the cached path at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcache_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and outgoing traffic has been\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    627\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m disabled. To enable file online look-ups, set \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal_files_only\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to False.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    628\u001b[0m     )\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m404\u001b[39m:\n\u001b[1;32m--> 630\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find file at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    631\u001b[0m _raise_if_offline_mode_is_enabled(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to reach \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_error \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Couldn't find file at https://dumps.wikimedia.org/swwiki/20220120/dumpstatus.json"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "l=load_dataset(\"wikipedia\", language=\"sw\", date=\"20220120\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "89eec421",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'l' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43ml\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'l' is not defined"
     ]
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573f68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
